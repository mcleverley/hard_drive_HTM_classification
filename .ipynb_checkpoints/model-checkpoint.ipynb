{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#data-loading,-preprocessing\" data-toc-modified-id=\"data-loading,-preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>data loading, preprocessing</a></span></li><li><span><a href=\"#parameters\" data-toc-modified-id=\"parameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>parameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#encoder-size\" data-toc-modified-id=\"encoder-size-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>encoder size</a></span></li><li><span><a href=\"#encoders-needed:-5?\" data-toc-modified-id=\"encoders-needed:-5?-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>encoders needed: 5?</a></span><ul class=\"toc-item\"><li><span><a href=\"#optional-strat:-drop-non-normalized-SMART-stats-and-try-again-(if-runtime-too-long),-since-we're-sort-of...-double-encoding,-almost\" data-toc-modified-id=\"optional-strat:-drop-non-normalized-SMART-stats-and-try-again-(if-runtime-too-long),-since-we're-sort-of...-double-encoding,-almost-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>optional strat: drop non-normalized SMART stats and try again (if runtime too long), since we're sort of... double encoding, almost</a></span></li></ul></li><li><span><a href=\"#experimental-encoder-hack-to-bypass-error\" data-toc-modified-id=\"experimental-encoder-hack-to-bypass-error-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>experimental encoder hack to bypass error</a></span></li></ul></li><li><span><a href=\"#SP-+-TM-setup\" data-toc-modified-id=\"SP-+-TM-setup-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>SP + TM setup</a></span></li><li><span><a href=\"#training-loop\" data-toc-modified-id=\"training-loop-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>training loop</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from htm.bindings.sdr import SDR, Metrics\n",
    "from htm.encoders.rdse import RDSE, RDSE_Parameters\n",
    "from htm.encoders.date import DateEncoder\n",
    "from htm.encoders import scalar_encoder\n",
    "from htm.encoders.scalar_encoder import ScalarEncoder, ScalarEncoderParameters\n",
    "from htm.bindings.algorithms import SpatialPooler\n",
    "from htm.bindings.algorithms import TemporalMemory\n",
    "from htm.algorithms.anomaly_likelihood import AnomalyLikelihood #FIXME use TM.anomaly instead, but it gives worse results than the py.AnomalyLikelihood now\n",
    "from htm.bindings.algorithms import Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mark/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reorganizing for \"lifetime\" view...\n",
      "cleaning indices...\n",
      "cleaning datatypes & coding categories to ints...\n",
      "adding failure horizons...\n",
      "failure indexes collected\n",
      "failure horizon indexes drawn\n",
      "final failure count: 2512\n",
      "adding failure horizons to data\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hard_disks.csv', \n",
    "                 index_col=0\n",
    "                )\n",
    "# df = df.drop(columns=['Unnamed: 0'])\n",
    "print('reorganizing for \"lifetime\" view...')\n",
    "df = df.sort_values(by=['date','serial_number']) # should it be serial_num then date? i wonder\n",
    "print('cleaning indices...')\n",
    "df = df.reset_index()\n",
    "df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# len(df.columns)\n",
    "print('cleaning datatypes & coding categories to ints...')\n",
    "cats = ['serial_number', 'model'] # categorical coding as ints for SDR encoding\n",
    "for cat in cats:\n",
    "    df[cat] = df[cat].astype('category')\n",
    "df['serial_code'] = df['serial_number'].cat.codes\n",
    "df['model_code'] = df['model'].cat.codes\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# failure horizons: source(Thomas Gaddy, machine learning for hard drive prediction failure)\n",
    "print('adding failure horizons...')\n",
    "fails = df[df['failure']==1]\n",
    "inds = list(fails.index)\n",
    "print('failure indexes collected')\n",
    "horizons = [] \n",
    "for ind in inds: # indexes with current failures\n",
    "    for b in range(-7, 0):\n",
    "        horizons.append(b+ind) # append the last 7 days\n",
    "    horizons.append(ind) # append the day\n",
    "print('failure horizon indexes drawn')\n",
    "for h in horizons:\n",
    "    # df.at or df.set_value is even faster than df.ix\n",
    "    df.at[h, 'failure'] = 1\n",
    "print('final failure count: ' + str(len(df[df['failure']==1])))\n",
    "print('added failure horizons to data.')\n",
    "print('||| Preprocessing complete |||')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters\n",
    "## encoder size\n",
    "encoder 'size' can be small since we're running 62 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "  # won't be using these exact params for each encoder, however.\n",
    " 'enc': {\n",
    "      \"value\" :\n",
    "         {'resolution': 0.88, 'size': 800, 'sparsity': 0.02},\n",
    "      \"time\": \n",
    "         {'timeOfDay': (30, 1), 'weekend': 21}\n",
    " },\n",
    " 'predictor': {'sdrc_alpha': 0.1},\n",
    " 'sp': {'boostStrength': 3.0,\n",
    "        'columnCount': 6000, # tweak this a little\n",
    "        'localAreaDensity': 0.04395604395604396,\n",
    "        'potentialPct': 0.85,\n",
    "        'synPermActiveInc': 0.04,\n",
    "        'synPermConnected': 0.13999999999999999,\n",
    "        'synPermInactiveDec': 0.006},\n",
    " 'tm': {'activationThreshold': 17,\n",
    "        'cellsPerColumn': 20,\n",
    "        'initialPerm': 0.21,\n",
    "        'maxSegmentsPerCell': 128,  # these two are good runtime/complexity tradeoffs to tweak\n",
    "        'maxSynapsesPerSegment': 64, # \"interconnection density\" of simulated slice of neocortex\n",
    "        'minThreshold': 10,\n",
    "        'newSynapseCount': 32,\n",
    "        'permanenceDec': 0.1,\n",
    "        'permanenceInc': 0.1},\n",
    " 'anomaly': {\n",
    "   'likelihood': \n",
    "       {#'learningPeriod': int(math.floor(self.probationaryPeriod / 2.0)),\n",
    "        #'probationaryPeriod': self.probationaryPeriod-default_parameters[\"anomaly\"][\"likelihood\"][\"learningPeriod\"],\n",
    "        'probationaryPct': 0.05, # tinker with this depending on size of dataset\n",
    "               # can't let it exceed historical window (check anomalyLikelihood.py binding)\n",
    "        'reestimationPeriod': 100} #These settings are copied from NAB\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoders needed: 5?\n",
    "- dateTime encoder to understand temporal sequence element\n",
    "- category encoder...? for the model\n",
    "    - could just convert each model to a number\n",
    "    - maybe RDSE does this for us, we'll see\n",
    "- scalar for capacity_bytes\n",
    "- one-bit encoder for 'failure' tacked on at the end, maybe.\n",
    "- SMART stat encoder\n",
    "\n",
    "maybe 4 encoders, we can use SMART_enc for capacity_bytes \n",
    "\n",
    "### optional strat: drop non-normalized SMART stats and try again (if runtime too long), since we're sort of... double encoding, almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2562\n"
     ]
    }
   ],
   "source": [
    "smarts = [col for col in list(df.columns) if 'smart' in col] # list of SMART stat column names\n",
    "\n",
    "date_encoder = DateEncoder(timeOfDay=(30,1), weekend=21) # less work is done on weekends\n",
    "# predict 'weekday vs weekend' HD consumption use to predict impending failure...?\n",
    "# maybe unnecessary, we'll see\n",
    "\n",
    "# SMART encoder\n",
    "stat_params = RDSE_Parameters()\n",
    "stat_params.size = 30 # this number is the one i'll be changing around the most\n",
    "stat_params.sparsity = 0.02 # the magic number\n",
    "stat_params.resolution = 0.88 \n",
    "stat_encoder = RDSE(stat_params) # create encoder\n",
    "# we could also use this for byte_capacity\n",
    "\n",
    "# failure 1-bit encoder to minimize SDR size. we only need one bit to encode 1 or 0, after all\n",
    "fail_params = ScalarEncoderParameters() \n",
    "fail_params.minimum = 0\n",
    "fail_params.maximum = 1\n",
    "fail_params.size = 30 # if i go under 30, it crashes: CHECK FAILED: \"args_.activeBits > 0u\" \n",
    "    # this is a pretty awful problem, considering i'm wasting 29 bits of calculation\n",
    "    # i'm probably missing some configuration in a setting somewhere\n",
    "fail_params.sparsity = 0.02\n",
    "# fail_params.resolution = 0.88\n",
    "fail_encoder = ScalarEncoder(fail_params) # it probably could just be a ScalarEncoder, not RDSE ...\n",
    "\n",
    "# # 5-bit scalar encoder for categorical \"model\" string, identifies unique hard disk.\n",
    "model_params = RDSE_Parameters() # in retrospect, naming one of the encoders 'model' isn't great nomenclature\n",
    "model_params.size = 30 # 5! will take care of 45 unique models\n",
    "model_params.sparsity = 0.02\n",
    "model_params.resolution = 0.88\n",
    "model_encoder = RDSE(model_params)\n",
    "\n",
    "# serial num encoder. 145,000 uniques\n",
    "serial_params = RDSE_Parameters() # in retrospect, naming one of the encoders 'model' isn't great nomenclature\n",
    "serial_params.size = 30 # 5! will take care of 45 unique models\n",
    "serial_params.sparsity = 0.02\n",
    "serial_params.resolution = 0.88\n",
    "serial_encoder = RDSE(serial_params)\n",
    "\n",
    "# calculate total bits in encoder\n",
    "# smart encoder is used 56 times (for SMART stats) and once for byte capacity\n",
    "encoder_width = date_encoder.size + stat_encoder.size*(len(smarts)+1) + fail_encoder.size \\\n",
    "+ model_encoder.size + serial_encoder.size # ah, i forgot that we use the stat encoder\n",
    "\n",
    "# encoder_width = date_encoder.size + encoder.size*(len(list(df.columns))-1)\n",
    "# encoder_info = Metrics([encoder_width], 999999999) # more 9s, the better\n",
    "\n",
    "print(encoder_width)\n",
    "\n",
    "# i was making a separate encoder for capacity_bytes, but it's 14 zeros and 30 bits from\n",
    "# STAT_encoder is already 10^32\n",
    "# mem_params = RDSE_Parameters\n",
    "# mem_params.size = 10 # max(df['capacity_bytes']) = 16000900661248. 14 zeros needs a deal of bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experimental encoder hack to bypass error\n",
    "- \"\"TM invalid input dimensions: \" << activeColumns.dimensions.size() << \" vs. \" << columnDimensions_.size();\"\n",
    "- let's test \"what if i ran every bloody variable through one encoder to assert proper width\"\n",
    "    - awful performance, but if it fixes the crash i'll be buggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_params = RDSE_Parameters()\n",
    "# enc_params.size = 40\n",
    "# enc_params.sparsity = 0.02\n",
    "# enc_params.resolution = 0.88\n",
    "# encoder = RDSE(enc_params)\n",
    "\n",
    "# date_encoder = DateEncoder(timeOfDay=(30,1), weekend=21) # less work is done on weekends\n",
    "\n",
    "# encoder_width = (encoder.size * (len(list(df.columns))-1)) + date_encoder.size\n",
    "# encoder_info = Metrics([encoder_width], 999999999) # it's 9 9s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP + TM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial pooler\n",
    "spParams = parameters['sp']\n",
    "sp = SpatialPooler(\n",
    "    inputDimensions            = (encoder_width,),\n",
    "    columnDimensions           = (spParams[\"columnCount\"],),\n",
    "    potentialPct               = spParams[\"potentialPct\"],\n",
    "    potentialRadius            = encoder_width,\n",
    "    globalInhibition           = True,\n",
    "    localAreaDensity           = spParams[\"localAreaDensity\"],\n",
    "    synPermInactiveDec         = spParams[\"synPermInactiveDec\"],\n",
    "    synPermActiveInc           = spParams[\"synPermActiveInc\"],\n",
    "    synPermConnected           = spParams[\"synPermConnected\"],\n",
    "    boostStrength              = spParams[\"boostStrength\"],\n",
    "    wrapAround                 = True\n",
    ")\n",
    "sp_info = Metrics(sp.getColumnDimensions(), 999999999)\n",
    "\n",
    "# temporal memory to help model understand 'lifetime' progression of hard drive\n",
    "tmParams = parameters['tm']\n",
    "tm = TemporalMemory(\n",
    "    columnDimensions          = (spParams[\"columnCount\"],),\n",
    "    cellsPerColumn            = tmParams[\"cellsPerColumn\"], # 13\n",
    "    activationThreshold       = tmParams[\"activationThreshold\"], # 17\n",
    "    initialPermanence         = tmParams[\"initialPerm\"], # 0.21\n",
    "    connectedPermanence       = spParams[\"synPermConnected\"],\n",
    "    minThreshold              = tmParams[\"minThreshold\"], # 19\n",
    "    maxNewSynapseCount        = tmParams[\"newSynapseCount\"], # 32\n",
    "    permanenceIncrement       = tmParams[\"permanenceInc\"], # 0.1\n",
    "    permanenceDecrement       = tmParams[\"permanenceDec\"], # 0.1\n",
    "    predictedSegmentDecrement = 0.0,\n",
    "    maxSegmentsPerCell        = tmParams[\"maxSegmentsPerCell\"], # 128\n",
    "    maxSynapsesPerSegment     = tmParams[\"maxSynapsesPerSegment\"] # 64\n",
    ")\n",
    "tm_info = Metrics( [tm.numberOfCells()], 999999999)\n",
    "\n",
    "# anomaly prediction\n",
    "anParams = parameters['anomaly']['likelihood']\n",
    "probationaryPeriod = int(math.floor(float(anParams['probationaryPct'])*len(df)))\n",
    "    # interesting error when we use len(df) to calculate probationaryPeriod\n",
    "    # estimationSamples is probPeriod - learningPeriod.\n",
    "        # learningPeriod is floordiv half of probPeriod\n",
    "    # so if the data is quite large (12.5mil in this case), we have \n",
    "learningPeriod = int(math.floor(probationaryPeriod / 2.0))\n",
    "anomaly_history = AnomalyLikelihood(learningPeriod = learningPeriod,\n",
    "       estimationSamples = probationaryPeriod - learningPeriod, # default 100.\n",
    "        historicWindowSize = int((probationaryPeriod - learningPeriod) * 1.2),\n",
    "        # when i tried manually adjusting historicWindowSize to be 20% larger than estimationSamples\n",
    "        # (because it crashes if HWS < ES) have to manually recast as int\n",
    "       reestimationPeriod = anParams['reestimationPeriod'])\n",
    "# change for 'depth' of future predictions\n",
    "future_length = [i+1 for i in range(3)]\n",
    "predictor = Predictor(steps=future_length, alpha=parameters['predictor']['sdrc_alpha'])\n",
    "predictor_resolution = 1 # divisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97751b2c1363428582d27c7068398621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2562\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CHECK FAILED: \"dimensions_ == data.dimensions\" ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3e69c6cf1d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mencoder_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# pass encoding into spatial pooler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CHECK FAILED: \"dimensions_ == data.dimensions\" "
     ]
    }
   ],
   "source": [
    "print('training...')\n",
    "\n",
    "# training parameters\n",
    "\n",
    "# future_length = [i+1 for i in range(3)] \n",
    "    # how 'deep' into the future to predict\n",
    "    # this is set before when we initialize the predictor object (after sp+tm)\n",
    "learning_thresh = len(df)/300 # number of rows after which to begin predicting\n",
    "# input and anomaly holders\n",
    "inputs = []\n",
    "anomaly = []\n",
    "anomalyProb = []\n",
    "# prediction dictionary\n",
    "lengths = [i+1 for i in range(len(future_length))]\n",
    "holders = [[] for l in lengths]\n",
    "predictions = dict(zip(lengths, holders))\n",
    "leng = len(df)\n",
    "\n",
    "predictor.reset()\n",
    "for i, row in tqdm_notebook(df.iterrows()): # row-by-row feed \n",
    "    failure = row['failure'] # y-variable, target, label, etc.\n",
    "        # create encoding, piece by piece then concatenate\n",
    "    # dateTime\n",
    "    pieces = []\n",
    "    \n",
    "    date = date_encoder.encode(row['date'])\n",
    "    pieces.append(date)\n",
    "    # serial number\n",
    "    serial = serial_encoder.encode(row['serial_code'])\n",
    "    pieces.append(serial)\n",
    "    # failure\n",
    "    fail = fail_encoder.encode(row['failure'])\n",
    "    pieces.append(fail)\n",
    "    # model number\n",
    "    model = model_encoder.encode(row['model_code'])\n",
    "    pieces.append(model)\n",
    "    # byte capacity\n",
    "    capacity = stat_encoder.encode(row['capacity_bytes']) # use SMART stat encoder for this one due to numerical similarity\n",
    "    pieces.append(capacity)\n",
    "    # SMART statistics\n",
    "    for stat in smarts: # 56 of these, raw and normalized\n",
    "        stat_encoding = stat_encoder.encode(row[stat])\n",
    "        pieces.append(stat_encoding)\n",
    "        \n",
    "    # string each bit-encoded feature into a final encoding SDR\n",
    "#     ls = pieces\n",
    "#     break\n",
    "    wid = 0\n",
    "    for piece in pieces:\n",
    "        wid += piece.size\n",
    "#     experimental_width = []\n",
    "    encoding = SDR(wid).concatenate(pieces)\n",
    "    print(encoding.size)\n",
    "    a = encoding\n",
    "    encoder_info.addData(encoding)\n",
    "    \n",
    "    # pass encoding into spatial pooler\n",
    "    active_cols = SDR(sp.getColumnDimensions())\n",
    "    sp.compute(encoding, True, active_cols) # learn = True\n",
    "    \n",
    "    # feed pooled SDR into temporal memory\n",
    "#     neuron_state = tm.getActiveCells() # i wonder if this has to be compressed into one line\n",
    "    # or if the previous bug was due to the encoding being too large compared to SP size\n",
    "    tm_info.addData(tm.getActiveCells().flatten())\n",
    "    tm.compute(active_cols, learn=True) # once i accidentally passed tm.getActiveCells() instead of active SP columns\n",
    "    # that was a terrific headache, let me tell you\n",
    "    \n",
    "    if i < learning_thresh: # if TM hasn't seen enough data to make a good prediction yet\n",
    "        continue # skip to the next row of loop\n",
    "    \n",
    "    # predict X steps into future\n",
    "    post_learn_neuron_state = tm.getActiveCells() # separate for clarity\n",
    "    pdf = predictor.infer(post_learn_neuron_state)\n",
    "    for n in tuple(future_length):\n",
    "        if pdf[n]: # if prediction was made\n",
    "            predictions[n].append(np.argmax(pdf[n])*predictor_resolution) # *1\n",
    "        else:\n",
    "            predictions[n].append(float('nan')) # no prediction\n",
    "    # anomaly calculation. associate target variable with temporal memory anomaly metric\n",
    "    anomaly_likelihood = anomaly_history.anomalyProbability(failure, tm.anomaly)\n",
    "    anomaly.append(tm.anomaly)\n",
    "    anomalyProb.append(anomaly_likelihood)\n",
    "    \n",
    "    # teach the predictor\n",
    "    predictor.learn(i, tm.getActiveCells(), int(failure/predictor_resolution))\n",
    "    \n",
    "    if failure == 1:    # experimental: if disk lifetime ends. tm.reset() to signal 'end of unique sequence'\n",
    "        tm.reset()        # the sequence being 'that particular disk's lifetime'.\n",
    "    \n",
    "    # end loop\n",
    "print(' ||| Training complete |||')\n",
    "    \n",
    "    # don't forget to predictor.reset() when you reach the end of one disk's lifetime\n",
    "    \n",
    "    # can't we skip the pre-training by saying \"only predict if i > len(df)/30?\"\n",
    "    # that should take care of 'must call learn before infer' errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = [i+1 for i in range(10)]\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10% chance of each of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['failure']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after re-calculating the width of each SDR_piece (summing) passing it to the full SDR, now we're failing on encoder_info.addData\n",
    "    - to instantiate that encoder_info object, we used the previous encoder_width\n",
    "    - so i'm pretty sure this whole thing is a problem with how i calculated the exact encoder width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CHECK FAILED: \"concat_axis_size == dimensions[axis]\" Axis of concatenation dimensions do not match, inputs sum to 2562, output expects 3242!\n",
    " - this one's interesting. output expects 3242... axis of concat dimensions don't match\n",
    " - the crash is something to do with stringing encoded features together into the final SDR\n",
    " - probably missed or miscalculated encoder_width and a few feature encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHECK FAILED: \"activeColumns.dimensions.size() == columnDimensions_.size()\" TM invalid input dimensions: 2 vs. 1\n",
    "what an interesting error. activeCols.dimensions.size != columnDimensions=.size()\n",
    "TM invalid input dimensions... 2 vs 1? what on earth is going on there?\n",
    "- i looked through the c++ source files. i'd guess it's something to do with my encoding width pre-defined and post-defined? maybe not.\n",
    "    - compared the code with my other HTM models. turns out i accidentally passed tm.getActiveCells() to tm.compute() instead of active spatial pooler columns.\n",
    "    - instead of feeding the TM the pooled algorithm, I fed it itself. no wonder it crashed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_encoder = DateEncoder(timeOfDay=(30,1), weekend=21) # less work is done on weekends\n",
    "# predict 'weekday vs weekend' HD consumption use to predict impending failure...?\n",
    "# maybe unnecessary, we'll see\n",
    "\n",
    "# SMART encoder\n",
    "stat_params = RDSE_Parameters()\n",
    "stat_params.size = 30 # this number is the one i'll be changing around the most\n",
    "stat_params.sparsity = 0.02 # the magic number\n",
    "stat_params.resolution = 0.88 \n",
    "stat_encoder = RDSE(stat_params) # create encoder\n",
    "# we could also use this for byte_capacity. model\n",
    "\n",
    "# failure 1-bit encoder to minimize SDR size. we only need one bit to encode 1 or 0, after all\n",
    "fail_params = ScalarEncoderParameters() \n",
    "fail_params.minimum = 0\n",
    "fail_params.maximum = 1\n",
    "fail_params.size = 30 # if i go under 30, it crashes: CHECK FAILED: \"args_.activeBits > 0u\" \n",
    "    # this is a pretty awful problem, considering i'm wasting 29 bits of calculation\n",
    "    # i'm probably missing some configuration in a setting somewhere\n",
    "fail_params.sparsity = 0.02\n",
    "# fail_params.resolution = 0.88\n",
    "fail_encoder = ScalarEncoder(fail_params) # it probably could just be a ScalarEncoder, not RDSE ...\n",
    "\n",
    "# # 5-bit scalar encoder for categorical \"model\" string, identifies unique hard disk.\n",
    "model_params = RDSE_Parameters() # in retrospect, naming one of the encoders 'model' isn't great nomenclature\n",
    "model_params.size = 30 # 5! will take care of 45 unique models\n",
    "model_params.sparsity = 0.02\n",
    "model_params.resolution = 0.88\n",
    "model_encoder = RDSE(model_params)\n",
    "\n",
    "# serial num encoder. 145,000 uniques\n",
    "serial_params = RDSE_Parameters() # in retrospect, naming one of the encoders 'model' isn't great nomenclature\n",
    "serial_params.size = 30 # 5! will take care of 45 unique models\n",
    "serial_params.sparsity = 0.02\n",
    "serial_params.resolution = 0.88\n",
    "serial_encoder = RDSE(serial_params)\n",
    "\n",
    "# calculate total bits in encoder\n",
    "# smart encoder is used 56 times (for SMART stats) and once for byte capacity\n",
    "encoder_width = date_encoder.size + stat_encoder.size*(len(smarts)+1) + fail_encoder.size \\\n",
    "+ model_encoder.size + serial_encoder.size\n",
    "encoder_info = Metrics([encoder_width], 999999999) # more 9s, the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart_1_normalized',\n",
       " 'smart_1_raw',\n",
       " 'smart_2_normalized',\n",
       " 'smart_2_raw',\n",
       " 'smart_3_normalized',\n",
       " 'smart_3_raw',\n",
       " 'smart_4_normalized',\n",
       " 'smart_4_raw',\n",
       " 'smart_5_normalized',\n",
       " 'smart_5_raw',\n",
       " 'smart_7_normalized',\n",
       " 'smart_7_raw',\n",
       " 'smart_8_normalized',\n",
       " 'smart_8_raw',\n",
       " 'smart_9_normalized',\n",
       " 'smart_9_raw',\n",
       " 'smart_10_normalized',\n",
       " 'smart_10_raw',\n",
       " 'smart_12_normalized',\n",
       " 'smart_12_raw',\n",
       " 'smart_184_normalized',\n",
       " 'smart_184_raw',\n",
       " 'smart_187_normalized',\n",
       " 'smart_187_raw',\n",
       " 'smart_188_normalized',\n",
       " 'smart_188_raw',\n",
       " 'smart_189_normalized',\n",
       " 'smart_189_raw',\n",
       " 'smart_190_normalized',\n",
       " 'smart_190_raw',\n",
       " 'smart_191_normalized',\n",
       " 'smart_191_raw',\n",
       " 'smart_192_normalized',\n",
       " 'smart_192_raw',\n",
       " 'smart_193_normalized',\n",
       " 'smart_193_raw',\n",
       " 'smart_194_normalized',\n",
       " 'smart_194_raw',\n",
       " 'smart_195_normalized',\n",
       " 'smart_195_raw',\n",
       " 'smart_196_normalized',\n",
       " 'smart_196_raw',\n",
       " 'smart_197_normalized',\n",
       " 'smart_197_raw',\n",
       " 'smart_198_normalized',\n",
       " 'smart_198_raw',\n",
       " 'smart_199_normalized',\n",
       " 'smart_199_raw',\n",
       " 'smart_200_normalized',\n",
       " 'smart_200_raw',\n",
       " 'smart_240_normalized',\n",
       " 'smart_240_raw',\n",
       " 'smart_241_normalized',\n",
       " 'smart_241_raw',\n",
       " 'smart_242_normalized',\n",
       " 'smart_242_raw']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "714.858px",
    "left": "1472.46px",
    "top": "53.4233px",
    "width": "160.98px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
